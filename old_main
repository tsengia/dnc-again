
def main():
    global i
    global loss_sum
    global running
    parser = ArgumentParser()
    parser.add_argument("-bit_w", type=int, default=8, help="Bit vector length for copy task")
    parser.add_argument("-block_w", type=int, default=3, help="Block width to associative recall task")
    parser.add_argument("-len", type=str, default="4", help="Sequence length for copy task", parser=lambda x: [int(a) for a in x.split("-")])
    parser.add_argument("-repeat", type=str, default="1", help="Sequence length for copy task", parser=lambda x: [int(a) for a in x.split("-")])
    parser.add_argument("-batch_size", type=int, default=16, help="Sequence length for copy task")
    parser.add_argument("-n_subbatch", type=str, default="auto", help="Average this much forward passes to a backward pass")
    parser.add_argument("-max_input_count_per_batch", type=int, default=6000, help="Max batch_size*len that can fit into memory")
    parser.add_argument("-lr", type=float, default=0.0001, help="Learning rate")
    parser.add_argument("-wd", type=float, default=1e-5, help="Weight decay")
    parser.add_argument("-optimizer", type=str, default="rmsprop", help="Optimizer algorithm")
    parser.add_argument("-name", type=str, help="Save training to this directory")
    parser.add_argument("-preview_interval", type=int, default=10, help="Show preview every nth iteration")
    parser.add_argument("-info_interval", type=int, default=10, help="Show info every nth iteration")
    parser.add_argument("-save_interval", type=int, default=500, help="Save network every nth iteration")
    parser.add_argument("-masked_lookup", type=bool, default=1, help="Enable masking in content lookups")
    parser.add_argument("-visport", type=int, default=-1, help="Port to run Visdom server on. -1 to disable")
    parser.add_argument("-gpu", default="auto", type=str, help="Run on this GPU.")
    parser.add_argument("-debug", type=bool, default=1, help="Enable debugging")
    parser.add_argument("-task", type=str, default="copy", help="Task to learn")
    parser.add_argument("-mem_count", type=int, default=16, help="Number of memory cells")
    parser.add_argument("-data_word_size", type=int, default=128, help="Memory word size")
    parser.add_argument("-n_read_heads", type=int, default=1, help="Number of read heads")
    parser.add_argument("-layer_sizes", type=str, default="256", help="Controller layer sizes. Separate with ,. For example 512,256,256", parser=lambda x: [int(y) for y in x.split(",") if y])
    parser.add_argument("-debug_log", type=bool, default=0, help="Enable debug log")
    parser.add_argument("-controller_type", type=str, default="lstm", help="Controller type: lstm or linear")
    parser.add_argument("-lstm_use_all_outputs", type=bool, default=1, help="Use all LSTM outputs as controller output vs use only the last layer")
    parser.add_argument("-momentum", type=float, default=0.9, help="Momentum for optimizer")
    parser.add_argument("-embedding_size", type=int, default=256, help="Size of word embedding for NLP tasks")
    parser.add_argument("-test_interval", type=int, default=10000, help="Run test in this interval")
    parser.add_argument("-dealloc_content", type=bool, default=1, help="Deallocate memory content, unlike DNC, which leaves it unchanged, just decreases the usage counter, causing problems with lookup")
    parser.add_argument("-sharpness_control", type=bool, default=1, help="Distribution sharpness control for forward and backward links")
    parser.add_argument("-think_steps", type=int, default=0, help="Iddle steps before requiring the answer (for bAbi)")
    parser.add_argument("-dump_profile", type=str, save=False)
    parser.add_argument("-test_on_start", default="0", save=False)
    parser.add_argument("-dump_heatmaps", default=False, save=False)
    parser.add_argument("-test_batch_size", default=16)
    parser.add_argument("-mask_min", default=0.0)
    parser.add_argument("-load", type=str, save=False)
    parser.add_argument("-dataset_path", type=str, default="none", parser=ArgumentParser.str_or_none(), help="Specify babi path manually")
    parser.add_argument("-babi_train_tasks", type=str, default="none", parser=ArgumentParser.list_or_none(type=str), help="babi task list to use for training")
    parser.add_argument("-babi_test_tasks", type=str, default="none", parser=ArgumentParser.list_or_none(type=str), help="babi task list to use for testing")
    parser.add_argument("-babi_train_sets", type=str, default="train", parser=ArgumentParser.list_or_none(type=str), help="babi train sets to use")
    parser.add_argument("-babi_test_sets", type=str, default="test", parser=ArgumentParser.list_or_none(type=str), help="babi test sets to use")
    parser.add_argument("-noargsave", type=bool, default=False, help="Do not save modified arguments", save=False)
    parser.add_argument("-demo", type=bool, default=False, help="Do a single step with fixed seed", save=False)
    parser.add_argument("-exit_after", type=int, help="Exit after this amount of steps. Useful for debugging.", save=False)
    parser.add_argument("-grad_clip", type=float, default=10.0, help="Max gradient norm")
    parser.add_argument("-clip_controller", type=float, default=20.0, help="Max gradient norm")
    parser.add_argument("-print_test", default=False, save=False)

    parser.add_profile([
        ArgumentParser.Profile("babi", {
            "preview_interval": 10,
            "save_interval": 500,
            "task": "babi",
            "mem_count": 256,
            "data_word_size": 64,
            "n_read_heads": 4,
            "layer_sizes": "256",
            "controller_type": "lstm",
            "lstm_use_all_outputs": True,
            "momentum": 0.9,
            "embedding_size": 128,
            "test_interval": 5000,
            "think_steps": 3,
            "batch_size": 2
        }),

        ArgumentParser.Profile("repeat_copy", {
            "bit_w": 8,
            "repeat": "1-8",
            "len": "2-14",
            "task": "copy",
            "think_steps": 1,
            "preview_interval": 10,
            "info_interval": 10,
            "save_interval": 100,
            "data_word_size": 16,
            "layer_sizes": "32",
            "n_subbatch": 1,
            "controller_type": "lstm",
        }),

        ArgumentParser.Profile("repeat_copy_simple", {
            "repeat": "1-3",
        }, include="repeat_copy"),

        ArgumentParser.Profile("dnc", {
            "masked_lookup": False,
            "sharpness_control": False,
            "dealloc_content": False
        }),

        ArgumentParser.Profile("dnc-m", {
            "masked_lookup": True,
            "sharpness_control": False,
            "dealloc_content": False
        }),

        ArgumentParser.Profile("dnc-s", {
            "masked_lookup": False,
            "sharpness_control": True,
            "dealloc_content": False
        }),

        ArgumentParser.Profile("dnc-d", {
            "masked_lookup": False,
            "sharpness_control": False,
            "dealloc_content": True
        }),

        ArgumentParser.Profile("dnc-md", {
            "masked_lookup": True,
            "sharpness_control": False,
            "dealloc_content": True
        }),

        ArgumentParser.Profile("dnc-ms", {
            "masked_lookup": True,
            "sharpness_control": True,
            "dealloc_content": False
        }),

        ArgumentParser.Profile("dnc-sd", {
            "masked_lookup": False,
            "sharpness_control": True,
            "dealloc_content": True
        }),

        ArgumentParser.Profile("dnc-msd", {
            "masked_lookup": True,
            "sharpness_control": True,
            "dealloc_content": True
        }),

        ArgumentParser.Profile("keyvalue", {
            "repeat": "1",
            "len": "2-16",
            "mem_count": 16,
            "task": "keyvalue",
            "think_steps": 1,
            "preview_interval": 10,
            "info_interval": 10,
            "data_word_size": 32,
            "bit_w": 12,
            "save_interval": 1000,
            "layer_sizes": "32"
        }),

        ArgumentParser.Profile("keyvalue2way", {
            "task": "keyvalue2way",
        }, include="keyvalue"),

        ArgumentParser.Profile("associative_recall",{
            "task": "recall",
            "bit_w": 8,
            "len": "2-16",
            "mem_count": 64,
            "data_word_size": 32,
            "n_read_heads": 1,
            "layer_sizes": "128",
            "controller_type": "lstm",
            "lstm_use_all_outputs": 1,
            "think_steps": 1,
            "mask_min": 0.1,
            "info_interval": 10,
            "save_interval": 1000,
            "preview_interval": 10,
            "n_subbatch": 1,
        })
    ])

    opt = parser.parse()
    assert opt.name is not None, "Training dir (-name parameter) not given"
    opt = parser.sync(os.path.join(opt.name, "args.json"), save=not opt.noargsave)

    embedding = None
    test_set = None
    if opt.task=="copy":
        dataset = CopyData(bit_w=opt.bit_w)
        in_size = opt.bit_w + 1
        out_size = in_size
    elif opt.task=="recall":
        dataset = AssociativeRecall(bit_w=opt.bit_w, block_w=opt.block_w)
        in_size = opt.bit_w + 2
        out_size = in_size
    elif opt.task=="keyvalue":
        assert opt.bit_w % 2==0, "Key-value datasets works only with even bit_w"
        dataset = KeyValue(bit_w=opt.bit_w)
        in_size = opt.bit_w + 1
        out_size = opt.bit_w//2
    elif opt.task=="keyvalue2way":
        assert opt.bit_w % 2==0, "Key-value datasets works only with even bit_w"
        dataset = KeyValue2Way(bit_w=opt.bit_w)
        in_size = opt.bit_w + 2
        out_size = opt.bit_w//2
    elif opt.task=="babi":
        dataset = bAbiDataset(think_steps=opt.think_steps, dir_name=opt.dataset_path)
        test_set = bAbiDataset(think_steps=opt.think_steps, dir_name=opt.dataset_path, name="test")
        dataset.use(opt.babi_train_tasks, opt.babi_train_sets)
        in_size = opt.embedding_size
        print("bAbi: loaded total of %d sequences." % len(dataset))
        test_set.use(opt.babi_test_tasks, opt.babi_test_sets)
        out_size = len(dataset.vocabulary)
        print("bAbi: using %d sequences for training, %d for testing" % (len(dataset), len(test_set)))
    else:
        assert False, "Invalid task: %s" % opt.task

    #if opt.task in ["babi"]:
        # data_loader = torch.utils.data.DataLoader(dataset, batch_size=opt.batch_size, num_workers=4, pin_memory=True, shuffle=True, collate_fn=MetaCollate())
        # test_loader = torch.utils.data.DataLoader(test_set, batch_size=opt.test_batch_size, num_workers=opt.test_batch_size, pin_memory=True, shuffle=False, collate_fn=MetaCollate()) if test_set is not None else None
    #else:
    dataset = BitmapTaskRepeater(dataset)
        # data_loader = torch.utils.data.DataLoader(dataset, batch_sampler=LengthHackSampler(opt.batch_size, BitmapTaskRepeater.key_sampler(opt.len, opt.repeat)), num_workers=1, pin_memory=True)

    if opt.controller_type == "lstm":
        controller_constructor = functools.partial(LSTMController, out_from_all_layers=opt.lstm_use_all_outputs)
    elif opt.controller_type == "linear":
        controller_constructor = FeedforwardController
    else:
        assert False, "Invalid controller: %s" % opt.controller_type

    model = DNC(in_size, out_size, opt.data_word_size, opt.mem_count, opt.n_read_heads, controller_constructor(opt.layer_sizes),
                batch_first=True, mask=opt.masked_lookup, dealloc_content=opt.dealloc_content,
                link_sharpness_control=opt.sharpness_control,
                mask_min=opt.mask_min, clip_controller=opt.clip_controller)

    params = [
        {'params': [p for n, p in model.named_parameters() if not n.endswith(".bias")]},
        {'params': [p for n, p in model.named_parameters() if n.endswith(".bias")], 'weight_decay': 0}
    ]

    device = torch.device("cpu")
    print("DEVICE: ", device)

    if isinstance(dataset, NLPTask):
        embedding = torch.nn.Embedding(len(dataset.vocabulary), opt.embedding_size).to(device)
        params.append({'params': embedding.parameters(), 'weight_decay': 0})

    if opt.optimizer=="sgd":
        optimizer = torch.optim.SGD(params, lr=opt.lr, weight_decay=opt.wd, momentum=opt.momentum)
    elif opt.optimizer=="adam":
        optimizer = torch.optim.Adam(params, lr=opt.lr, weight_decay=opt.wd)
    elif opt.optimizer == "rmsprop":
        optimizer = torch.optim.RMSprop(params, lr=opt.lr, weight_decay=opt.wd, momentum=opt.momentum, eps=1e-10)
    else:
        assert "Invalid optimizer: %s" % opt.optimizer

    print(optimizer)
    n_params = sum([sum([t.numel() for t in d['params']]) for d in params])
    print("Number of parameters: %d" % n_params)
